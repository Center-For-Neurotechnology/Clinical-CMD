% -----------------------------------------------------------------------
% Inputs:
%
% STR: A string corresponding to a patient group. Accepted inputs include
% 'pe', 'pl', and 'c'.
%
% TASK: A string corresponding to a task type (aka stim type). If input is
% invalid, will prompt until valid input is supplied.
%
% SPLIT_TASK: A flag indicating splitting condition: all trials (0), the
% first six trials (1) or the latter six trials (2).
%
% SPLIT_TRL is an optional flag indicating whether spectra will be computed
% for the full duration of each trial (0), the first half of ON/second half
% of OFF (1), the last half of ON/first half of OFF (2), the first half of
% ON/first half of OFF (3), last half of ON/last half of OFF (4). Default
% behavior: use the whole trial.
%
% REJ_FLAG: A string specifying the type of artifact rejection
% corresponding to the data. Used to build and search directories for data
% from a particular AR condition.
%
% CV_FLAG: A string specifying the cross validation scheme to be used to
% build the classifier. 'Kfold' and 'LOO' are acceptable inputs; however,
% only 'Kfold' will give interpretable results as the current
% implementation of leave-one-out cross validation is invalid.
%
% ITER is a double indicating the number of iterations to run for the
% permutation test.
%
% N_K is an optional value containing a number of folds to use for cross
% validation. If not specified, function assigns a value to N_K on the
% basis of # trials contained in the data matrix.
%
% Builds a classifier based on a concatenated matrix of trials of dim n_sec
% by [freq band by elec] in ON and OFF condition for a given blocked task
% (Music, Language, or Motor). This matrix comes from ABSOLUTE band power
% values averaged over a frequency band and generated by
% C_POWERSPECTRA_EPOCHED_CS.
%
% Uses n_k fold cross validation to build the classifier (NOTE: While
% leave-one-out cross validation is theoretically available in this
% function, as of 7/22/2015 it has not been correctly implemented, so it
% should NOT BE USED). If not provided, decides the value of n_k on the
% basis of data duration by attempting to match the number of folds (n_k)
% to the number of trials present in the data, which varies depending on
% the splitting conditions and that stimulus type, with the goal of
% approximating the leave-one-out approach.
%
% Obtains p-values for statistical significance by a permuatation using the
% method described in Noirhomme 2014. In brief, a classifier is trained
% using the same scheme as the one described above, except with the true
% labels mixed up. This is repeated ITER number of times, and the p-value
% is obtained by counting the number of times that the dummy classifier
% obtained a higher accuracy than the true classifier.

% --------------------------
% Authors: Camille Spencer, 2015; Matteo Fecchio 2022
% --------------------------


function [class]=EEGClass_v1(specON,specOFF,badCH,f,chanlocs,n_k,iter,n_rep)
% n_k=20;
% iter=500;
% n_rep=10;
freq_bands = {[1,3],[4,7],[8,13],[14,30]};
Ifreqs=cellfun(@(x) find(abs(f-x(1))==min(abs(f-x(1)))): ...
    find(abs(f-x(2))==min(abs(f-x(2)))),freq_bands,'UniformOutput',false);
goodCH=setdiff(1:length(chanlocs),badCH);

% Generate features by averaging power over bands instead of using
% power at each electrode.
ON=cell2mat(cellfun(@(x) squeeze(mean(specON(x,:,:),1)),Ifreqs,'UniformOutput',false));
OFF=cell2mat(cellfun(@(x) squeeze(mean(specOFF(x,:,:),1)),Ifreqs,'UniformOutput',false));

group_labels = [ones(size(ON,1),1); zeros(size(OFF,1),1)];
data = [ON; OFF];

class_recall = NaN(1,n_rep);
class_precision = NaN(1,n_rep);
class_accuracy = NaN(1,n_rep);
pooled_wt_all = NaN(n_rep, length(goodCH),n_k);
pooled_wt_avg = NaN(n_rep, length(goodCH),1);
pooled_wt_avg_freq = NaN(n_rep, length(freq_bands), length(goodCH));
h = waitbar(0,'Wait...');
for jj = 1:n_rep
    cp = classperf(group_labels); % Create and initialize an empty classifier performance object.
    ind = crossvalind('Kfold',group_labels,n_k); % Generate indices for splitting data
    w_all = NaN(size(data,2),n_k);
    test=cellfun(@(x) (ind == x),num2cell(1:n_k),'UniformOutput',false);
    train=cellfun(@(x) ~x,test,'UniformOutput',false);
    warning('off','all')
   
    SVMModel=cellfun(@(x) svmtrain(data(x,:),group_labels(x,:),...
        'Kernel_Function','linear','Method','LS', 'BoxConstraint', 1),train,'UniformOutput',false);

    est_labels =cellfun(@(x,y) svmclassify(x,data(y,:)),SVMModel,test,'UniformOutput',false);
    
    for k = 1:n_k % For n_k folds
        classperf(cp,est_labels{k},test{k});
        w_all(:,k) = SVMModel{k}.Alpha'*SVMModel{k}.SupportVectors; % Save weights for each fold
    end
    w_avg = mean(w_all,2); % Save average of all folds
    % Pool weights from all frequency bands per electrode to
    % get one weight per electrode.
    
    pooled_wt_all(jj,:,:)=squeeze(mean(reshape(w_all,[length(freq_bands) length(goodCH) n_k]),1));
    pooled_wt_avg(jj,:)=mean(reshape(w_avg,[length(freq_bands) length(goodCH)]),1);
    pooled_wt_avg_freq(jj,:,:)=reshape(w_avg,[length(freq_bands) length(goodCH)]);
    class_accuracy(jj) = cp.CorrectRate;
    class_precision(jj)=cp.PositivePredictiveValue;
    class_recall(jj)=cp.Sensitivity;
    waitbar(jj/n_rep)
end
close(h)
class_precision = mean(class_precision);
class_recall = mean(class_recall);
class_accuracy = mean(class_accuracy);
pooled_wt_all = squeeze(mean(pooled_wt_all,1));
pooled_wt_avg = mean(pooled_wt_avg,1);
pooled_wt_avg_freq = squeeze(mean(pooled_wt_avg_freq,1));
% pooled_wt_all_abs = squeeze(mean(pooled_wt_all_abs,1));
% pooled_wt_avg_abs = mean(pooled_wt_avg_abs,1);
% pooled_wt_avg_freq_abs = squeeze(mean(pooled_wt_avg_freq_abs,1));
clear k jj cp SVMModel test train

figure;topoplot(mean(abs(pooled_wt_avg_freq),1),chanlocs(goodCH))
caxis([-0.3 0.3])
% figure;topoplot(abs(pooled_wt_avg),chanlocs(goodCH))
% caxis([-0.3 0.3])
hfig=gca;
% figure,
% freqsname={'Delta','Theta','Alpha','Beta'};
% for kk=1:4
%     subplot(1,4,kk)
%     topoplot(pooled_wt_avg_freq(kk,:),chanlocs(goodCH))
%     title(freqsname{kk})
%     caxis([-0.2 0.2])
% end
%% Permutation test - do the exact same thing, but mix up the
% labels for each iteration of the classifier.
h = waitbar(0,'Permutation test...');
for x = 1:iter
    mixed_group_labels = group_labels(randperm(size(data,1)));
    
    cp = classperf(group_labels);   % Ground truth is preserved by initializing w/true group labels.
    ind = crossvalind('Kfold',group_labels,n_k);
    
    test=cellfun(@(x) (ind == x),num2cell(1:n_k),'UniformOutput',false);
    train=cellfun(@(x) ~x,test,'UniformOutput',false);
    warning('off','all')
    
    SVMModel=cellfun(@(x) svmtrain(data(x,:),mixed_group_labels(x,:),...
        'Kernel_Function','linear','Method','LS', 'BoxConstraint', 1),train,'UniformOutput',false);
    est_labels =cellfun(@(x,y) svmclassify(x,data(y,:)),SVMModel,test,'UniformOutput',false);
    for k = 1:n_k
        classperf(cp,est_labels{k},test{k});
    end
    
    dummy_accuracy(x) = cp.CorrectRate;
    dummy_class_precision(x)=cp.PositivePredictiveValue;
    dummy_class_recall(x)=cp.Sensitivity;
    clear cp ind mixed_group_labels
    waitbar(x/iter)
end
close (h)
% Get p-value according to method described in Noirhomme 2014.
class.n_rep = n_rep;
class.folds = n_k;
class.all_iter = iter;
class.p_value_plus1 = (sum(dummy_accuracy>=class_accuracy)+1)/(iter+1);
class.p_value = sum(dummy_accuracy>=class_accuracy)/iter;

class.accuracy = class_accuracy;
class.precision=class_precision;
class.recall=class_recall;
class.dur = sum(~isnan(ON(:,1)))+sum(~isnan(OFF(:,1)));
class.pooled_wt_all=pooled_wt_all;
class.pooled_wt_avg=pooled_wt_avg;
class.pooled_wt_avg_freq=pooled_wt_avg_freq;
title(hfig,['pvalue:' num2str(round(class.p_value_plus1*1000)/1000) ' - class accuracy:' num2str(round(class.accuracy*100)/100)])
